{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbzGZ5nx93iY"
   },
   "source": [
    "### PyTorch\n",
    "\n",
    "Facultad de Ingeniería - Universidad de la República - Uruguay\n",
    "\n",
    "Setiembre de 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qrgAvcoi93io"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pln-fing-udelar/cursos/blob/master/tutoriales/PyTorch-Intro.ipynb)\n",
    "\n",
    "\n",
    "En este notebook presentaremos una breve introducción a las características principales de PyTorch, una biblioteca para deep learning en Python. Este módulo asume que el lector conoce NumPY, la biblioteca de análisis numérico básico en Python, y scikit-learn, la biblioteca de aprendizaje automático más popular en el lenguaje (la utilizaremos para funciones auxiliares de carga de datos y alguna tarea más, pero no para el aprendizaje propiamente dicho).\n",
    "\n",
    "\n",
    "Referencias:\n",
    "\n",
    "- El notebook está basado principalmente en el libro [Machine Learning with PyTorch and Scikit-Learn](https://github.com/rasbt/machine-learning-book), de Sebastian Raschka, Yuxi Liu y Vahid Mirjalili, en particular los capítulos 12 y 13\n",
    "- La documentación de PyTorch tienen un [tutorial](https://pytorch.org/tutorials/beginner/basics/intro.html) mostrando cómo entrenar una red neuronal que clasifica prendas, entrenado sobre el dataset FashionMNIST\n",
    "- Hay una muy breve [introducción](https://docs.pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html) a autograd en la documentación de PyTorch.\n",
    "- Para una introducción paso a paso al uso de PyTorch para DL, el curso de Sebastian Raschka [Deep Learning fundamentals](https://lightning.ai/courses/deep-learning-fundamentals/) es un material excelente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xzp0A71393iw",
    "outputId": "822600e6-138c-4b84-f826-d5d2a1ba06a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versión de Torch: 2.2.2+cpu\n",
      "Versión de Numpy: 1.24.3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Este notebook fue elaborado con la versión 2.0.1 de Torch y la versión 1.24.3 de NumPy\n",
    "print(\"Versión de Torch:\",torch.__version__)\n",
    "print(\"Versión de Numpy:\",np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zk1gZeXk93jY"
   },
   "source": [
    "## 1. Tensores\n",
    "\n",
    "Las estructuras básicas de PyTorch son los _tensores_. Los tensores son generalizaciones de los escalares, vectores, matrices, etc. Un escalar es un tensor de rango 0, un vector es un tensor de rango 1, y una matriz es un tensor de rango 2. Los tensores, sin embargo, pueden tener rangos arbitrariamente grandes.  Los tensores son parecidos a los arrays de PyTorch, pero están optimizados para las operaciones de diferenciación automática y pueden correr en GPUs (volveremos sobre esto). Los elementos de un tensor (igual que como sucede con los arrays de NumPy y a diferencia de las listas de Python) tienen un tipo de datos únicos. Es importante tener claro el tipo (y es una buena práctica especificarlo al crear el tensor), ya que puede hacer una diferencia importante cuando se manejan volúmenes grandes de datos (por ejemplo, un elemento de tipo `torch.float64` ocupará el doble de espacio que uno de tipo `torch.float32`\n",
    "\n",
    "\n",
    "Los tensores pueden crearse a partir de listas de Python o arrays de NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5PHF30gMabpH",
    "outputId": "e4b71562-ba97-4c00-8c05-584956983773"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3], dtype=torch.int32)\n",
      "tensor([3., 4., 5.])\n"
     ]
    }
   ],
   "source": [
    "t_a = torch.tensor([1,2,3], dtype=torch.int32)\n",
    "print(t_a)\n",
    "\n",
    "b = np.array([3,4,5], dtype=np.float32)\n",
    "t_b = torch.from_numpy(b)\n",
    "print(t_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiQJ7PXTabpJ"
   },
   "source": [
    "Podemos consultar las propiedades de un tensor, tales como tipo de datos, y su forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mIs9X2a2abpJ",
    "outputId": "0aae8b3b-fdc9-40f3-9ce5-60262f322ff4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4426, 0.3929, 0.2000, 0.7114],\n",
      "        [0.9016, 0.0849, 0.1904, 0.7641],\n",
      "        [0.9752, 0.6045, 0.4169, 0.2851]])\n",
      "torch.Size([3, 4])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "t_c = torch.rand(3,4) # El tipo por defecto será torch.float32\n",
    "print(t_c)\n",
    "\n",
    "# El tensor tiene tres filas y 4 columnas\n",
    "print(t_c.shape)\n",
    "print(t_c.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWoVqxfeabpK"
   },
   "source": [
    "Los tensores pueden dinámicamente cambiar su forma (i.e. cuántos valores tiene cada dimensión). PyTorch resuelve de forma \"inteligente\" estos cambios, siempre que se mantenga la cantidad de elementos. El siguiente ejemplo utiliza el método `reshape` para  transformar un tensor que es un vector de 10 elementos, en una matriz de 2 filas por 5 columnas (existe un método similar, `view`, que cumple la misma función, y que sólo difiere en que exige que los datos estén en bloques contiguos de memoria)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZMT3_rxcabpL",
    "outputId": "1caec6b0-d805-4bc3-87bb-bf7d1b2355d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t_d = torch.ones(10)\n",
    "t_d1 = t_d.reshape(2,5) # La convención para el orden es, igual que en numpy, fila/columna\n",
    "print(t_d)\n",
    "print(t_d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3J0iirz7abpN"
   },
   "source": [
    "Si una de las dimensiones de reshape vale -1, entonces PyTorch resuelve de forma inteligente el valor, para que el total de elementos sea el adecuado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-80V_K_FabpO",
    "outputId": "a0519648-a0ca-4e93-a0ae-a36e0d279ecf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t_d2 = t_d.reshape(5,-1) # 5 filas, las columnas que sean necesarias\n",
    "print(t_d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKk-F96btbDl"
   },
   "source": [
    "Existe un atributo muy relevante para los tensores, `device`. Este atributo indica en qué dispositivo se almacena (y por lo tanto, se procesa).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gUQO47vzt6Px",
    "outputId": "65185a9f-48d7-4241-8c15-7afc984cb24b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo actual: cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"Dispositivo actual: {}\".format(t_a.device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7bYbEl3uiLm"
   },
   "source": [
    "Si tenemos disponible una GPU, podemos decirle a PyTorch que el tensor la utilice, simplemente modificando el valor del atributo `device`. Debemos previamente verificar que es posible, porque de lo contrario nos dará un error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KS1uaIHjwDXo",
    "outputId": "ff1b4a7a-f031-42ea-c8dd-d38a1e7628c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Esto funciona porque este notebook está corriendo en una GPU\n",
    "if torch.cuda.is_available():\n",
    "  t_a = t_a.to(torch.device(\"cuda\"))\n",
    "\n",
    "print(t_a.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AiDzSCW-yjix"
   },
   "source": [
    "Usualmente, lo que hacemos es, al comienzo del notebook, chequear si hay disponible una CPU y asignar a los tensores una GPU si existe, o una cpu, en caso contrario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9VWR3V8uvTt",
    "outputId": "1c1cea3c-8afc-47c9-90f9-85a8aec43650"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU (T4) is not available, using CPU instead.\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU (T4) is available and will be used.\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\") # Get the name of the first GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU (T4) is not available, using CPU instead.\")\n",
    "\n",
    "t = torch.tensor([1,1,1], device=device)\n",
    "print (t.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCL83PebabpQ"
   },
   "source": [
    "## 2. Operaciones con Tensores\n",
    "\n",
    "Igual que numpy permite operar con arrays, PyTorch utiliza prácticamente las mismas reglas y operaciones para realizar operaciones matemáticas sobre los tensores.\n",
    "\n",
    "El siguiente ejemplo multiplica elemento a elemento dos tensores de tamaño 2x3 (el primero, creado a partir de números aleatorios con una distribución uniforme en \\[0,1), y el segundo con una distribución normal de media 0 y desviación estándar 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VEmYJbw-abpQ",
    "outputId": "631c73bb-4faa-4001-c023-b53a6c31b7f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4581, 0.4829],\n",
      "        [0.3125, 0.6150],\n",
      "        [0.2139, 0.4118],\n",
      "        [0.6938, 0.9693],\n",
      "        [0.6178, 0.3304]])\n",
      "tensor([[-1.0531,  0.9457],\n",
      "        [-0.8307, -0.3713],\n",
      "        [ 1.2137, -0.6678],\n",
      "        [-0.4038, -0.1300],\n",
      "        [ 0.7410, -0.1425]])\n",
      "tensor([[-0.4824,  0.4567],\n",
      "        [-0.2596, -0.2284],\n",
      "        [ 0.2597, -0.2750],\n",
      "        [-0.2802, -0.1260],\n",
      "        [ 0.4578, -0.0471]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(10) # Es buena costumbre fijar la semilla para que los resultados sean reproducibles\n",
    "t1 = torch.rand(5,2)\n",
    "t2 = torch.normal(mean=0,std=1,size=(5,2))\n",
    "\n",
    "print(t1)\n",
    "print(t2)\n",
    "\n",
    "print(torch.multiply(t1,t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iu-8NUOuabpR"
   },
   "source": [
    "Podemos también hacer cálculos sobre ciertas axes (axis=0 son las filas, axis=1 las columnas, y así sucesivamente). Por ejamplo, para calcular la media de cada una de las columnas del segundo tensor con números aleatorios, utilizamos `torch.mean`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A3QlvCMOabpR",
    "outputId": "47e496eb-fd15-4706-ecd3-3bf50e9da5bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0666, -0.0732])\n"
     ]
    }
   ],
   "source": [
    "print(torch.mean(t2,axis=0)) # Debería dar algo cercano a 0..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4xU_zeUabpS"
   },
   "source": [
    "Para calcular multiplicaciones entre matrices, se utiliza `torch.matmul`, que realiza las operaciones [usuales](https://es.wikipedia.org/wiki/Multiplicaci%C3%B3n_de_matrices) (también puede utilizarse el operador `@` para lograr exactamente el mismo resultado)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rb52wKrtabpS",
    "outputId": "e1a29f4c-b795-4a96-fb12-11761e5ffcba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[19, 22],\n",
      "        [43, 50]])\n"
     ]
    }
   ],
   "source": [
    "t4 = torch.tensor([[1,2],[3,4]])\n",
    "t5 = torch.tensor([[5,6],[7,8]])\n",
    "\n",
    "print(torch.matmul(t4,t5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QrfQ_UqjabpT"
   },
   "source": [
    "Si queremos calcular la [norma](https://es.wikipedia.org/wiki/Norma_vectorial) de un tensor, utilizaremos `torch.linalg.norm`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gwkoa6kAabpT",
    "outputId": "bca1c63c-19ea-4e95-cd75-b33888d74775"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.9523)\n",
      "tensor(52.4014)\n"
     ]
    }
   ],
   "source": [
    "t6 = torch.rand(100)\n",
    "print(torch.linalg.norm(t6,ord=2))\n",
    "print(torch.linalg.norm(t6,ord=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cvLEqNVSabpU"
   },
   "source": [
    "## 3. Carga de datos\n",
    "\n",
    "Habiendo visto las operaciones básicas sobre tensores, veamos cómo entrenar una red neuronal utilizando PyTorch. Antes vamos a describir algunas funciones útiles que PyTorch provee para manejar datasets, especialmente en aquellos casos donde no podemos guardar todo el dataset en memoria, y deberemos irlo recuperando en pedazos (o batchs) desde disco, así como realizar otras funciones de preprocesamiento antes del aprendizaje propiamente dicho.\n",
    "\n",
    "Para mantener separado el código que procesa los datos del que realiza el entrenamiento, existen dos primitivas fundamentales: `torch.utils.data.Dataset` que permite manejar las instancias de datos y sus correspondientes etiquetas, y `torch.utils.data.DataLoader` que permite construir un interable a partir de un Dataset. El tipo de Dataset más básico es simplemente un tensor, y podemos crear un DataLoader a partir de él fácilmente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ex49u9YSabpU"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "t = torch.rand(20)\n",
    "data_loader = DataLoader(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbJw4iT9abpV"
   },
   "source": [
    "Habiendo construido el dataset, podemos iterar fácilmente sobre él:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w-l8immSabpV",
    "outputId": "7184ee07-f666-40fa-a8d4-3b93bfa81dc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4385])\n",
      "tensor([0.5048])\n",
      "tensor([0.3810])\n",
      "tensor([0.4269])\n",
      "tensor([0.1977])\n",
      "tensor([0.1699])\n",
      "tensor([0.6641])\n",
      "tensor([0.9510])\n",
      "tensor([0.1006])\n",
      "tensor([0.0280])\n",
      "tensor([0.2296])\n",
      "tensor([0.9799])\n",
      "tensor([0.9500])\n",
      "tensor([0.0135])\n",
      "tensor([0.6213])\n",
      "tensor([0.5674])\n",
      "tensor([0.9417])\n",
      "tensor([0.3501])\n",
      "tensor([0.6649])\n",
      "tensor([0.2524])\n"
     ]
    }
   ],
   "source": [
    "for item in data_loader:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-MCsnDlabpW"
   },
   "source": [
    "Podemos especificar que los elementos se procesen en batchs de cierto tamaño:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pHpOpUKvabpW",
    "outputId": "063ae75b-4961-45dd-f8f6-a29e41223651"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1: tensor([0.4385, 0.5048, 0.3810, 0.4269, 0.1977, 0.1699])\n",
      "batch 2: tensor([0.6641, 0.9510, 0.1006, 0.0280, 0.2296, 0.9799])\n",
      "batch 3: tensor([0.9500, 0.0135, 0.6213, 0.5674, 0.9417, 0.3501])\n",
      "batch 4: tensor([0.6649, 0.2524])\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader(t,batch_size=6,drop_last=False) # No descartamos el último batch si está incompleto\n",
    "for i, batch in enumerate(data_loader,1):\n",
    "    print(f'batch {i}:', batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Q863yQIabpX"
   },
   "source": [
    "Es muy común que tengamos dos tensores que nos interese procesar al mismo tiempo. El caso típico es un tensor con las features, y otro con las etiquetas. Para eso, la clase `TensorDataset` (que es una tipo especial de `torch.utils.data.Dataset`, que PyTorch permite definir de forma genérica) nos permite manejarlos como una unidad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZvSHd_SbabpX",
    "outputId": "6bbf183e-678c-4956-fde7-af14de706a60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([0.3896, 0.0956, 0.7927])  y: tensor(0)\n",
      "x: tensor([0.8453, 0.8823, 0.5649])  y: tensor(1)\n",
      "x: tensor([0.0279, 0.1002, 0.4475])  y: tensor(2)\n",
      "x: tensor([0.4481, 0.0462, 0.8099])  y: tensor(3)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Creamos un mini dataset que tiene 4 elementos y 3 atributos por cada uno\n",
    "t_x = torch.rand([4,3],dtype=torch.float32)\n",
    "t_y = torch.arange(4)\n",
    "joint_dataset = TensorDataset(t_x,t_y)\n",
    "\n",
    "for example in joint_dataset:\n",
    "    print('x:', example[0], ' y:', example[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "723ukL5iabpX"
   },
   "source": [
    "Con este dataset, podemos, por supuesto, crear un nuevo DataLoader. En este caso, vamos a especificarle, además, que cada vez que recorra el dataset, mezcle las instancias de forma aleatoria. Esto será muy útil cuando recorramos varias veces el conjunto de entrenamiento para entrenar la red neuronal. El método `manual_seed` permite especificar una semilla para el generador de datos aleatorios, parque los resultados puedan ser reproducibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "G-rOPmoPabpY"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "data_loader = DataLoader(dataset=joint_dataset, batch_size=2, shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2lQg6E0XabpY"
   },
   "source": [
    "Realizamos una iteración..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w-vPNgQmabpY",
    "outputId": "555ac0ab-0de4-49a0-bf7e-c42c67c20a5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1: [tensor([[0.3896, 0.0956, 0.7927],\n",
      "        [0.0279, 0.1002, 0.4475]]), tensor([0, 2])]\n",
      "batch 2: [tensor([[0.8453, 0.8823, 0.5649],\n",
      "        [0.4481, 0.0462, 0.8099]]), tensor([1, 3])]\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(data_loader,1):\n",
    "    print(f'batch {i}:', batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iETBLWO9abpa"
   },
   "source": [
    "Y luego otra, sobre el mismo dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZDnAJuUfabpa",
    "outputId": "3a1603b8-a85c-45fd-ebfe-6a83fd3812ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1: [tensor([[0.4481, 0.0462, 0.8099],\n",
      "        [0.0279, 0.1002, 0.4475]]), tensor([3, 2])]\n",
      "batch 2: [tensor([[0.8453, 0.8823, 0.5649],\n",
      "        [0.3896, 0.0956, 0.7927]]), tensor([1, 0])]\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(data_loader,1):\n",
    "    print(f'batch {i}:', batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKIsMU9Zabpa"
   },
   "source": [
    "## 4. Cargando datos con torchvision\n",
    "\n",
    "Los métodos de `Dataset` y `DataLoader` permite construir datasets de forma muy general, a partir de nuestros datos, utilizando PyTorch. Sin embargo, para los datasets más populares, existen bibliotecas como `TorchVision` o `TorchText` que ya los proveen. Veamos, por ejemplo, cómo importar el dataset [MNIST](http://yann.lecun.com/exdb/mnist/) de números escritos a mano, utilizando `torchvision`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Ow6Qb_Kabpa",
    "outputId": "0e812e51-9b8b-4e65-9619-1af466b67ef6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 9912422/9912422 [00:01<00:00, 7502405.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST\\raw\\train-images-idx3-ubyte.gz to ./MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 28881/28881 [00:00<00:00, 218031.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST\\raw\\train-labels-idx1-ubyte.gz to ./MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 1648877/1648877 [00:00<00:00, 1933182.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 4542/4542 [00:00<00:00, 1368670.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "image_path = './'\n",
    "\n",
    "# Esto nos permite convertir las features de los píxeles en tensores y los normaliza al rango [0,1]\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Importamos los datasets\n",
    "# Las etiquetas son valores entre 0 y 9 para representar los dígitos.\n",
    "mnist_train_dataset = torchvision.datasets.MNIST(root=image_path, train=True, transform=transform, download=True)\n",
    "mnist_test_dataset  = torchvision.datasets.MNIST(root=image_path, train=False, transform=transform, download=True)\n",
    "\n",
    "batch_size = 64\n",
    "torch.manual_seed(10)\n",
    "train_dl = DataLoader(mnist_train_dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4DhO8KuMabpb"
   },
   "source": [
    "## 5. El modelo de computación de PyTorch y autograd\n",
    "\n",
    "PyTorch permite construir _grafos de computación_ para derivar las relaciones entre los tensores, cuando se realizan operaciones entre ellos (como veremos más adelante, una red neuronal no es más que una serie de cálculos sobre tensores, partiendo del tensor inicial con la entrada).\n",
    "\n",
    "Empecemos con un ejemplo bien sencillo, que utiliza solamente escalares (i.e. tensores de dimensión 0). Supongamos que $a$, $b$ y $c$ son tensores y que  $ z = 2(a^2-b^3) +c$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "RJgyhl7Babpb",
    "outputId": "ee355535-30d2-48a4-af0c-6f1f2bca371c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-49.0360)\n"
     ]
    }
   ],
   "source": [
    "a=torch.tensor(2.5)\n",
    "b=torch.tensor(3.2)\n",
    "c=torch.tensor(4)\n",
    "\n",
    "# z es simplemente la operación aritmética \"clasica\" entre números reales.\n",
    "# Al asignar z, hacemos el cálculo (es lo que llamamos pasada forward por el grafo de computación\n",
    "z= 2*(a**2-b**3)+c\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDPK8Mhaabpb"
   },
   "source": [
    "Hasta aquí, nada que no podamos hacer con NumPy (de hecho... con Python). Pero PyTorch incorpora la capacidad de diferenciación automática (a través de una biblioteca llamada `autograd`), los que nos permite llevar registro de las derivadas del resultado respecto de los parámetros involucrados (al vector con estas derivadas se le llama _gradiente_, y es clave para minimizar la pérdida al entrenar redes, como veremos más adelante). Intentemos hacer lo mismo que antes, pero ahora indicamos a PyTorch que queremos calcular el gradiente de z respecto a a, b y c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QMdDaIx3abpc",
    "outputId": "70716a21-65ce-400c-e463-f1f4bf21a9d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-49.0360, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a=torch.tensor(2.5, requires_grad=True)\n",
    "b=torch.tensor(3.2, requires_grad=True)\n",
    "# Indicamos que c es un tensor de reales, porque solamente se puede calcular el gradiente\n",
    "# de números reales o complejos\n",
    "c=torch.tensor(4., requires_grad=True)\n",
    "\n",
    "z= 2*(a**2-b**3)+c\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWAawxj_abpc"
   },
   "source": [
    "El tensor z ahora \"sabe\" cómo fue calculado, porque autograd lleva el registro. Con esto podemos utilizar el método `backward` sobre z, para obtener el gradiente, utilizando backpropagation. Hagamos la prueba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ew1fggobabpe",
    "outputId": "1ccec6a7-da38-4177-e1e8-16d849312c94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.)\n",
      "tensor(-61.4400)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Aquí está toda la magia\n",
    "# Utiliza backpropagation para calcular el gradiente del resultado respecto a cada uno de los\n",
    "# parámetros por los que estamos controlando\n",
    "z.backward()\n",
    "print(a.grad)\n",
    "print(b.grad)\n",
    "print(c.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEpdlPICabpe"
   },
   "source": [
    "(Es interesante notar que podemos llamar una sola vez a este método, porque, para ahorrar memoria, PyTorch \"pierde\" el grafo de cómputo al finalizar el proceso de cálculo, salvo que especifiquemos `retain_graph=True` al invocarlo.\n",
    "\n",
    "Como esta es una función muy sencilla, podemos calcular directamente las derivadas y verificar los cálculos.\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial z}{\\partial a} = 4a = 10 \\\\\n",
    "\\frac{\\partial z}{\\partial b} = -6b = -61.44 \\\\\n",
    "\\frac{\\partial z}{\\partial c} =1\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHIo94Vbabpe"
   },
   "source": [
    "Veamos un nuevo caso, ahora un poco más parecido al entrenamiento una red neuronal, pero que utiliza exactamente los mismos principios.\n",
    "\n",
    "Supongamos que tenemos una red con una sola neurona, lineal, que toma dos valores $x_1$ y $x_2$ de entrada, lo multiplica por los respectivos pesos $w_1$ y $w_2$ y le suma un sesgo $b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ps21HkTVabpe",
    "outputId": "3fab476e-eb13-4a94-e538-0bd24602719d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.3300], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Nuestros parámetros son w y b\n",
    "# Los inicializamos con valores arbitrarios\n",
    "w = torch.tensor([0.5,0.3], requires_grad = True)\n",
    "b = torch.tensor([1.0], requires_grad = True)\n",
    "\n",
    "# Nuestra entrada es x, que representaremos por un tensor, al que no le calcularemos gradiente\n",
    "x = torch.tensor([1.4,2.1])\n",
    "\n",
    "# Calculamos z\n",
    "# Verificar que 2.33 = 0.5*1.4 + 0.3*2.1 + 1\n",
    "z = torch.dot(w,x) + b\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WLfSnhaabpg"
   },
   "source": [
    "Además, supongamos que tenemos una función de pérdida que compara el resultado con un objetivo conocido y cálcula su pérdida cuadrática"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-PjNr89oabpg",
    "outputId": "bca06895-1d5b-4aef-cc40-3d0f8b73c79f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5129, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = torch.tensor(1.1)\n",
    "loss = (y -z).pow(2).sum()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IX2oMlMSabph"
   },
   "source": [
    "Y ahora, la magia de backpropagation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "qt-hbr7aabph"
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XsUz9Qh7abpi",
    "outputId": "5356a683-f590-4389-924e-30d2fe6ffbad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/dw: tensor([3.4440, 5.1660])\n",
      "dL/db: tensor([2.4600])\n"
     ]
    }
   ],
   "source": [
    "print('dL/dw:',w.grad)\n",
    "print('dL/db:',b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAifmozWabpi"
   },
   "source": [
    "Podemos ver que el atributo `grad` de $w$ y de $b$ contiene el gradiente de la función de pérdida. Verificamos nuevamente (y por última vez...) que PyTorch hace bien las cuentas (lo haremos para el caso de $w_1$)\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial w_1} = \\frac{\\partial (z-y)^2}{\\partial w_1} = \\frac{\\partial (z-y)^2}{\\partial z}\\cdot \\frac{\\partial z}{\\partial w_1} = 2(z-y)\\cdot \\frac{x_1w_1 + x_2w_2 +b}{\\partial w_1} = 2(z-y)x_1 = 2(2.33-1.1)\\times1.4 = 3.444$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlG7TtRSabpj"
   },
   "source": [
    "## 6. nn.Sequential\n",
    "\n",
    "PyTorch provee una clase [`nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) que permite especificar redes feed forward de forma muy sencilla. Veamos un ejemplo:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XRCnhVMvabpj",
    "outputId": "f67fc57a-f5a9-4aca-d573-b3aeb485213e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=16, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=16, out_features=32, bias=True)\n",
       "  (3): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(4,16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16,32),\n",
    "    nn.ReLU()\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n0YyQXI2abpk"
   },
   "source": [
    "La red anterior recibe 4 reales como entrada y tiene dos capas: la primera tiene 16 nodos, y su función de activación es una ReLU, mientras que la segunda tiene 32 nodos y también una ReLU como salida. Por lo tanto, esta red tendrá como salida un valor real (y por lo tanto sería adecuada para un problema de regresión). Sobre este modelo, es posible especificar diferentes funciones de activación, aplicar regularizaciones, entre otras muchas funciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EeupDTPMabpk"
   },
   "source": [
    "## 8. Definición y entrenamiento de una red neuronal\n",
    "\n",
    "A partir de los datos de los dígitos de MNIST que importamos previamente, vamos a intentar predecir con una red neuronal las clases correspondientes.\n",
    "\n",
    "Primero, construimos el modelo de red (seguimos el ejemplo del libro de Rashka mencionado en la introducción):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6nJO6cb9abpl",
    "outputId": "d4172806-f7e8-4ca9-e159-608d3bd2d915"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Flatten(start_dim=1, end_dim=-1)\n",
      "  (1): Linear(in_features=784, out_features=32, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Linear(in_features=16, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "hidden_units = [32,16]\n",
    "image_size = mnist_train_dataset[0][0].shape\n",
    "input_size = image_size[0] * image_size[1] * image_size [2]\n",
    "\n",
    "# Nuestro primer paso es convertir las imágenes  a un vector\n",
    "all_layers = [nn.Flatten()]\n",
    "\n",
    "for hidden_unit in hidden_units:\n",
    "    layer = nn.Linear(input_size, hidden_unit)\n",
    "    all_layers.append(layer)\n",
    "    all_layers.append(nn.ReLU())\n",
    "    input_size = hidden_unit\n",
    "\n",
    "all_layers.append(nn.Linear(hidden_units[-1],10))\n",
    "\n",
    "model = nn.Sequential(*all_layers)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCqLAAlTabpl"
   },
   "source": [
    "A continuación, realizamos el entrenamiento. Para esto, especificamos la función de pérdida (en esta caso, Entropía cruzada) y el optimizador (Adam). Nos interesa usar entropía cruzada porque es la función de pérdida típica para softmax. El optimizador es quien se encarga de actualizar los parámetros a partir de los valores calculados del gradiente (el caso más conocido es descenso por gradiente).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MMINqJZSabpm",
    "outputId": "bc304a0a-2f23-4e5e-aa64-7fc705bd2c78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1fbb284d450>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "torch.manual_seed(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byVRN_5zabpm"
   },
   "source": [
    "Entrenamos por 20 épocas..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pq_x_oNAabpn",
    "outputId": "d600f468-a65f-4fab-c119-01258aa3a0bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Accuracy 0.8467\n",
      "Epoch 1  Accuracy 0.9297\n",
      "Epoch 2  Accuracy 0.9455\n",
      "Epoch 3  Accuracy 0.9534\n",
      "Epoch 4  Accuracy 0.9586\n",
      "Epoch 5  Accuracy 0.9633\n",
      "Epoch 6  Accuracy 0.9658\n",
      "Epoch 7  Accuracy 0.9684\n",
      "Epoch 8  Accuracy 0.9697\n",
      "Epoch 9  Accuracy 0.9731\n",
      "Epoch 10  Accuracy 0.9741\n",
      "Epoch 11  Accuracy 0.9755\n",
      "Epoch 12  Accuracy 0.9762\n",
      "Epoch 13  Accuracy 0.9779\n",
      "Epoch 14  Accuracy 0.9789\n",
      "Epoch 15  Accuracy 0.9799\n",
      "Epoch 16  Accuracy 0.9807\n",
      "Epoch 17  Accuracy 0.9817\n",
      "Epoch 18  Accuracy 0.9826\n",
      "Epoch 19  Accuracy 0.9836\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    accuracy_hist_train = 0\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        # Paso forward, obtenemos la predicción de la red con los valores actuales\n",
    "        pred = model(x_batch)\n",
    "        # Calculamos la pérdida.\n",
    "        loss = loss_fn(pred,y_batch)\n",
    "\n",
    "        # Backpropagation!\n",
    "        loss.backward()\n",
    "\n",
    "        # Ajustamos los pesos utilizando el algoritmo de optimización, en este caso Adam\n",
    "        optimizer.step()\n",
    "\n",
    "        # Ponemos los gradientes en zero\n",
    "        # Generalmente, los optimizadores mantienen el gradiente de la pasada anterior\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        is_correct = (torch.argmax(pred,dim=1) == y_batch).float()\n",
    "\n",
    "        accuracy_hist_train += is_correct.sum()\n",
    "\n",
    "    accuracy_hist_train /= len(train_dl.dataset)\n",
    "    print(f'Epoch {epoch}  Accuracy '\n",
    "          f'{accuracy_hist_train:.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IvrNJpgabpn"
   },
   "source": [
    "Nuestro modelo logró una accuracy de 98.36% sobre el conjunto de entrenamiento. Veamos ahora cómo es su performance sobre el conjunto de evaluación, para verificar que no hubo sobreajuste..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "KzneflRHabpn",
    "outputId": "7553f341-f03c-41a5-926b-036ff326401b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print((mnist_test_dataset.data).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "vhFku-ffabpo",
    "outputId": "0a0481eb-299d-48b2-ea83-6da3a9fddd89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9674\n"
     ]
    }
   ],
   "source": [
    "# Predecimos utilizando el modelo\n",
    "\n",
    "# Antes normalizamos los valores RGB\n",
    "# Esto lo hago porque no paso por el transform\n",
    "pred = model(mnist_test_dataset.data/255.)\n",
    "\n",
    "is_correct = ( torch.argmax(pred,dim=1) == mnist_test_dataset.targets).float()\n",
    "print(f'Test accuracy: {is_correct.mean():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YF7gIz5abpo"
   },
   "source": [
    "## 9. Más para leer\n",
    "\n",
    "En el capítulo 13 del libro explica (y hay código asociado) cómo utilizar la biblioteca Lightining Trainer y TensorBoard para mostrar mejor los resultados. El código está disponible en [github](https://github.com/rasbt/machine-learning-book/tree/main/ch13)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
